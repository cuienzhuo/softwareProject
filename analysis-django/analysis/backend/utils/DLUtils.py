import pandas as pd
import numpy as np
import joblib
from sklearn.metrics import mean_squared_error, mean_absolute_error
import tensorflow as tf
from tensorflow.keras.models import load_model
import os
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# --------------------------
# 1. é…ç½®å‚æ•°ä¸è·¯å¾„
# --------------------------
MODEL_PATH = r"E:\github01\softwareModel\DL_MODEL\traffic_prediction_model.keras"
SCALER_PATH = r"E:\github01\softwareModel\DL_MODEL\traffic_scaler.pkl"
CSV_PATH = r"E:\github01\softwareProject\analysis-django\analysis\backend\utils\milano_traffic_nid.csv"
LOOK_BACK = 60
INITIAL_WINDOW_SIZE = 3000


# --------------------------
# 2. åŠ è½½ä¾èµ–
# --------------------------
def load_dependencies():
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ {MODEL_PATH} ä¸å­˜åœ¨")
    model = load_model(MODEL_PATH)
    print(f"æˆåŠŸåŠ è½½æ¨¡å‹ï¼š{MODEL_PATH}")

    if not os.path.exists(SCALER_PATH):
        raise FileNotFoundError(f"å½’ä¸€åŒ–å™¨ {SCALER_PATH} ä¸å­˜åœ¨")
    scaler = joblib.load(SCALER_PATH)
    print(f"æˆåŠŸåŠ è½½å½’ä¸€åŒ–å™¨")

    if not os.path.exists(CSV_PATH):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ {CSV_PATH} ä¸å­˜åœ¨")
    df = pd.read_csv(CSV_PATH, index_col="timestamp", parse_dates=True)
    print(f"æˆåŠŸåŠ è½½æ•°æ®ï¼šå…± {len(df)} æ¡è®°å½•ï¼Œ{df.shape[1]} ä¸ªç‰¹å¾åˆ—")

    return model, scaler, df


# --------------------------
# 3. å‡†å¤‡æ»šåŠ¨æ•°æ®
# --------------------------
def prepare_rolling_data(df, roll_steps):
    total_needed = INITIAL_WINDOW_SIZE + roll_steps
    if total_needed > len(df):
        total_needed = len(df)
        adjusted_roll_steps = total_needed - INITIAL_WINDOW_SIZE
        print(f"è­¦å‘Šï¼šåŸå§‹æ•°æ®é•¿åº¦ä¸è¶³ï¼Œæ»šåŠ¨æ­¥æ•°è‡ªåŠ¨è°ƒæ•´ä¸º {adjusted_roll_steps}ï¼ˆåŸå§‹æ•°æ®å…± {len(df)} æ¡ï¼‰")
        return df.iloc[:total_needed], adjusted_roll_steps
    return df.iloc[:total_needed], roll_steps


# --------------------------
# 4. è®¡ç®— MAPEï¼ˆæ³¨æ„é¿å…é™¤é›¶ï¼‰
# --------------------------
def mean_absolute_percentage_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    # é¿å…é™¤ä»¥0ï¼šå½“çœŸå®å€¼ä¸º0æ—¶ï¼Œè·³è¿‡æˆ–è®¾ä¸º0è¯¯å·®ï¼ˆæ ¹æ®ä¸šåŠ¡é€»è¾‘ï¼‰
    mask = y_true != 0
    if not np.any(mask):
        return np.nan  # æˆ– 0.0ï¼Œè§†æƒ…å†µè€Œå®š
    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))
    return mape


# --------------------------
# 5. æ‰§è¡Œæ»šåŠ¨é¢„æµ‹å¹¶æ”¶é›†æ‰€æœ‰å¿…è¦æ•°æ®
# --------------------------
def run_rolling_test(model, scaler, df_test, roll_steps, num_features):
    predictions = []
    actuals = []
    step_rmse_list = []
    step_mae_list = []
    step_mape_list = []

    rolling_data = df_test.iloc[:INITIAL_WINDOW_SIZE].values
    rolling_data_scaled = scaler.transform(rolling_data)

    for i in range(roll_steps):
        input_seq = rolling_data_scaled[-LOOK_BACK:].reshape(1, LOOK_BACK, num_features)
        pred_scaled = model.predict(input_seq, verbose=0)[0]
        pred_actual = scaler.inverse_transform(pred_scaled.reshape(1, -1))[0]

        actual_idx = INITIAL_WINDOW_SIZE + i
        actual_actual = df_test.iloc[actual_idx].values

        # æ›´æ–°çª—å£ï¼ˆä½¿ç”¨çœŸå®å€¼ï¼‰
        actual_scaled = scaler.transform(actual_actual.reshape(1, -1))[0]
        rolling_data_scaled = np.append(
            rolling_data_scaled[1:],
            actual_scaled.reshape(1, -1),
            axis=0
        )

        predictions.append(pred_actual)
        actuals.append(actual_actual)

        # å•æ­¥æŒ‡æ ‡
        step_rmse = np.sqrt(mean_squared_error(actual_actual, pred_actual))
        step_mae = mean_absolute_error(actual_actual, pred_actual)
        step_mape = mean_absolute_percentage_error(actual_actual, pred_actual)

        step_rmse_list.append(float(step_rmse))
        step_mae_list.append(float(step_mae))
        step_mape_list.append(float(step_mape) if not np.isnan(step_mape) else None)

        if (i + 1) % 10 == 0:
            print(f"æ»šåŠ¨è¿›åº¦ï¼š{i + 1}/{roll_steps} | RMSE: {step_rmse:.2f}, MAE: {step_mae:.2f}")

    predictions = np.array(predictions)
    actuals = np.array(actuals)

    # æ•´ä½“æŒ‡æ ‡ï¼ˆæ•´ä¸ªåºåˆ—ï¼‰
    overall_rmse = float(np.sqrt(mean_squared_error(actuals, predictions)))
    overall_mae = float(mean_absolute_error(actuals, predictions))
    overall_mape = float(mean_absolute_percentage_error(actuals, predictions))

    return {
        "predictions": predictions.tolist(),
        "actuals": actuals.tolist(),
        "timestamps": df_test.index[INITIAL_WINDOW_SIZE: INITIAL_WINDOW_SIZE + roll_steps].strftime(
            "%Y-%m-%d %H:%M:%S").tolist(),
    }


# --------------------------
# 6. ä¸»å‡½æ•°ï¼šè¿”å›ç»“æ„åŒ–æ•°æ®ç»™å‰ç«¯
# --------------------------
def DLAnalysis(address, roll_steps):
    """
    è¿”å›ç»“æ„åŒ–é¢„æµ‹ç»“æœï¼Œä¾›å‰ç«¯ç»˜å›¾å’Œå±•ç¤ºæŒ‡æ ‡
    :param address: ç‰¹å¾åˆ—åï¼ˆå¦‚ 'nid_123'ï¼‰
    :param roll_steps: æ»šåŠ¨æ­¥æ•°
    :return: dict åŒ…å«é¢„æµ‹å€¼ã€çœŸå®å€¼ã€æ—¶é—´æˆ³ã€æŒ‡æ ‡
    """
    model, scaler, df = load_dependencies()

    if address not in df.columns:
        raise ValueError(f"æŒ‡å®šçš„ç‰¹å¾åˆ— '{address}' ä¸å­˜åœ¨äºæ•°æ®ä¸­ã€‚å¯ç”¨åˆ—ï¼š{list(df.columns)}")

    num_features = df.shape[1]
    df_test, roll_steps = prepare_rolling_data(df, roll_steps)

    full_result = run_rolling_test(model, scaler, df_test, roll_steps, num_features)

    # æå–ç›®æ ‡ç‰¹å¾ç´¢å¼•
    feature_idx = df.columns.get_loc(address)

    # æå–å•ç‰¹å¾åºåˆ—
    predictions_single = [p[feature_idx] for p in full_result["predictions"]]
    actuals_single = [a[feature_idx] for a in full_result["actuals"]]

    # é‡æ–°è®¡ç®—è¯¥ç‰¹å¾çš„ä¸‰å¤§æŒ‡æ ‡ï¼ˆæ›´å‡†ç¡®ï¼ï¼‰
    overall_rmse = float(np.sqrt(mean_squared_error(actuals_single, predictions_single)))
    overall_mae = float(mean_absolute_error(actuals_single, predictions_single))
    overall_mape = mean_absolute_percentage_error(actuals_single, predictions_single)
    overall_mape = float(overall_mape) if not np.isnan(overall_mape) else None

    # æ„é€ å‰ç«¯æ‰€éœ€ç»“æ„
    return {
        "chartData": {
            "timestamps": full_result["timestamps"],  # æ—¶é—´æˆ³å·²åœ¨ run_rolling_test ä¸­ç”Ÿæˆ
            "predictions": predictions_single,
            "actuals": actuals_single
        },
        "metrics": {
            "rmse": overall_rmse,
            "mae": overall_mae,
            "mape": overall_mape
        }
    }


def DLAnalysisWithoutPreTrain(address, train_ratio=0.8, look_back=60, epochs=50, batch_size=64, lstm_units=50):
    np.random.seed(42)
    tf.random.set_seed(42)

    try:
        df = pd.read_csv(CSV_PATH, index_col='timestamp', parse_dates=True)
    except FileNotFoundError:
        return {"error": "CSV file 'milano_traffic_nid.csv' not found."}

    df = df.dropna()
    print(f"æ•°æ®å½¢çŠ¶ï¼ˆå¤„ç†ç¼ºå¤±å€¼åï¼‰: {df.shape}")
    print(f"å¯ç”¨åˆ—: {df.columns.tolist()}")

    # è·å–ç›®æ ‡åˆ—ç´¢å¼•
    if address not in df.columns:
        return {"error": f"æŒ‡å®šçš„åœ°å€åˆ— '{address}' ä¸å­˜åœ¨äºæ•°æ®ä¸­ã€‚å¯ç”¨åˆ—: {df.columns.tolist()}"}
    target_idx = df.columns.get_loc(address)
    num_features = df.shape[1]

    # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
    train_size = int(len(df) * train_ratio)
    train_df = df.iloc[:train_size]
    test_df = df.iloc[train_size:]

    # å½’ä¸€åŒ–
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_train = scaler.fit_transform(train_df.values)
    scaled_test = scaler.transform(test_df.values)

    # æ•°æ®é›†ç”Ÿæˆå‡½æ•°ï¼šå¤šå˜é‡è¾“å…¥ â†’ å•å˜é‡è¾“å‡º
    def create_dataset_multivar_to_single(data, look_back, target_col_index):
        X, Y = [], []
        for i in range(len(data) - look_back):
            X.append(data[i:(i + look_back), :])
            Y.append(data[i + look_back, target_col_index])
        return np.array(X), np.array(Y)

    X_train, y_train = create_dataset_multivar_to_single(scaled_train, look_back, target_idx)
    X_test, y_test = create_dataset_multivar_to_single(scaled_test, look_back, target_idx)

    y_train = y_train.reshape(-1, 1)
    y_test = y_test.reshape(-1, 1)

    print(f"\nè®­ç»ƒé›†: X_train={X_train.shape}, y_train={y_train.shape}")
    print(f"æµ‹è¯•é›†: X_test={X_test.shape}, y_test={y_test.shape}")

    # æ„å»ºLSTMæ¨¡å‹ï¼ˆä½¿ç”¨ä¼ å…¥çš„ lstm_unitsï¼‰
    model = Sequential(name="Single_Target_LSTM")
    model.add(Input(shape=(look_back, num_features)))
    model.add(LSTM(lstm_units, return_sequences=True))
    model.add(LSTM(lstm_units))
    model.add(Dense(1))  # å•è¾“å‡º

    model.compile(loss='mse', optimizer='adam', metrics=['mae'])
    model.summary()

    # å›è°ƒå‡½æ•°
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),
        ModelCheckpoint(filepath='best_single_model.keras', monitor='val_loss', save_best_only=True, verbose=1)
    ]

    # è®­ç»ƒ
    history = model.fit(
        X_train, y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_data=(X_test, y_test),
        callbacks=callbacks,
        verbose=2
    )

    # é¢„æµ‹
    train_predict = model.predict(X_train, verbose=0)
    test_predict = model.predict(X_test, verbose=0)

    # åå½’ä¸€åŒ–è¾…åŠ©å‡½æ•°
    def inverse_transform_single(scaler, data, target_col_index):
        dummy = np.zeros((data.shape[0], num_features))
        dummy[:, target_col_index] = data.flatten()
        inversed = scaler.inverse_transform(dummy)
        return inversed[:, target_col_index]

    # åå½’ä¸€åŒ–
    train_predict_actual = inverse_transform_single(scaler, train_predict, target_idx)
    y_train_actual = inverse_transform_single(scaler, y_train, target_idx)
    test_predict_actual = inverse_transform_single(scaler, test_predict, target_idx)
    y_test_actual = inverse_transform_single(scaler, y_test, target_idx)

    # MAPE è®¡ç®—ï¼ˆé¿å…é™¤é›¶ï¼‰
    def mean_absolute_percentage_error(y_true, y_pred):
        y_true, y_pred = np.array(y_true), np.array(y_pred)
        non_zero = y_true != 0
        if not np.any(non_zero):
            return np.nan
        mape = np.mean(np.abs((y_true[non_zero] - y_pred[non_zero]) / y_true[non_zero]))
        return mape

    test_mae = mean_absolute_error(y_test_actual, test_predict_actual)
    test_rmse = np.sqrt(mean_squared_error(y_test_actual, test_predict_actual))
    test_mape = mean_absolute_percentage_error(y_test_actual, test_predict_actual)

    print("\n" + "=" * 50)
    print(f"ğŸ“Š {address} æ¨¡å‹è¯„ä¼°ç»“æœ")
    print("=" * 50)
    print(f"æµ‹è¯•é›† - MAE: {test_mae:.2f}, RMSE: {test_rmse:.2f}, MAPE: {test_mape:.2f}%")
    print("=" * 50)

    # === å‡†å¤‡è¿”å›ç»™å‰ç«¯çš„æ•°æ® ===
    test_time_full = test_df.index[look_back:]  # å¯¹åº” y_test_actual çš„æ—¶é—´æˆ³
    timestamps_full = test_time_full.strftime('%Y-%m-%d %H:%M:%S').tolist()
    actual_full = y_test_actual.tolist()
    predicted_full = test_predict_actual.tolist()

    # æœ€è¿‘48å°æ—¶ï¼ˆå‡è®¾æ¯è¡Œæ˜¯1å°æ—¶ï¼›è‹¥éå°æ—¶ç²’åº¦ï¼Œå¯è°ƒæ•´ HOURS_TO_SHOW å«ä¹‰ï¼‰
    HOURS_TO_SHOW = 192
    if len(timestamps_full) >= HOURS_TO_SHOW:
        recent_slice = slice(-HOURS_TO_SHOW, None)
    else:
        recent_slice = slice(None)

    result = {
        "metrics": {
            "mae": float(test_mae),
            "rmse": float(test_rmse),
            "mape": float(test_mape) if not np.isnan(test_mape) else None
        },
        "plots":{
            "main": {
                "timestamps": timestamps_full,
                "actual": actual_full,
                "predicted": predicted_full
            },
            "zoom": {
                "timestamps": timestamps_full[recent_slice],
                "actual": actual_full[recent_slice],
                "predicted": predicted_full[recent_slice]
            }
        }
    }

    print("\nâœ… æ•°æ®å·²å‡†å¤‡å®Œæ¯•ï¼Œå¯è¿”å›å‰ç«¯ç”¨äºç»˜å›¾ã€‚")
    return result